{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511be899-25e7-4cc8-a2ca-1b999b0e3557",
   "metadata": {},
   "source": [
    "# Fine Tune BERT for Q&A with Apple MLX\n",
    "\n",
    "and compare to PyTorch HuggingFace implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f8237-7e3d-4684-9900-330f31139e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4337f-4ecb-4630-94cc-db5e84bc7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f13de-862b-4f75-a34b-2d15d6c7e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9109b393-0060-4c3b-a3db-6567ca8a4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_processed_datasets\n",
    "from qa import load_model_tokenizer, batch_iterate, loss_fn, eval_fn\n",
    "\n",
    "# file from mlx repo\n",
    "from model_mlx import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987ef77-d384-469a-aeab-74b0053798e0",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97725a0e-1e29-4b58-a786-bcfb449e1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57227bd-38a2-4d68-badc-ad69d008f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for Bert()\n",
    "# batch = [\"This is an example of BERT working on MLX.\"]\n",
    "# tokens = tokenizer(batch, return_tensors=\"mlx\", padding=True)\n",
    "# output, pooled = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292a8b7-992c-481a-990e-555a2713b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_weights_path = \"weights/bert-base-uncased.npz\"\n",
    "model, tokenizer = load_model_tokenizer(hf_model=bert_model,\n",
    "                                        mlx_weights_path=mlx_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8d9dd-ecef-4c85-bf42-19be8ee93e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds, test_ds = load_processed_datasets(filter_size=100,\n",
    "                                            model_max_length=tokenizer.model_max_length, tokenizer=tokenizer)\n",
    "\n",
    "train_ds.shape, valid_ds.shape, test_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45914e2a-e792-4773-a4ec-5a6e6a58b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(batch_iterate(train_ds, batch_size=3))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06e0e9-1ade-4e8a-9a97-1bf0878507a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: mx.array() in preprocess_tokenize_function() ????\n",
    "input_ids, token_type_ids, attention_mask, start_positions, end_positions = map(\n",
    "    mx.array,\n",
    "    (batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['start_positions'], batch['end_positions'])\n",
    ")\n",
    "\n",
    "# input_ids = mx.expand_dims(input_ids, 0)\n",
    "# token_type_ids = mx.expand_dims(token_type_ids, 0)\n",
    "# attention_mask = mx.expand_dims(attention_mask, 0)\n",
    "\n",
    "input_ids.shape, token_type_ids.shape, attention_mask.shape, input_ids.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457b87c-c1a6-43d0-81a1-5bc03dfc1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_positions.shape, end_positions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79117641-3720-483b-91f8-54392e5cc9c9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bde1bc-9fe0-4b2c-851a-cc6ca9d31b34",
   "metadata": {},
   "source": [
    "follow transformer_lm/main/py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5c6ec-fe0a-443a-aa05-fdd955e8f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c24f7-ac4c-46ac-a3ba-8046accb10a0",
   "metadata": {},
   "source": [
    "### Train on single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03015069-242a-4024-bdba-f4491502c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d0650-1e91-4789-804b-eb89c4cfcf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95d02c-949f-4282-9964-002fa77a5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits, end_logits = model(\n",
    "    input_ids=input_ids,\n",
    "    token_type_ids=token_type_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    start_positions=start_positions,\n",
    "    end_positions=end_positions) \n",
    "\n",
    "a = nn.losses.cross_entropy(start_logits, start_positions)\n",
    "b = nn.losses.cross_entropy(end_logits, end_positions)\n",
    "\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c75c8-a0ec-4790-87d7-bb12fb7274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152c813-a373-4988-9229-513d2ffaca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model, input_ids, token_type_ids, attention_mask, start_positions, end_positions, reduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8afeb8-9210-40fa-99ba-0debaed0ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model, input_ids, token_type_ids, attention_mask, start_positions, end_positions, reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4455b85-169b-4e94-ba26-2f08fd553229",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece8f5d-7d3c-404d-93d8-ab2311912d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grads = loss_and_grad_fn(model, input_ids, token_type_ids, attention_mask, start_positions, end_positions)\n",
    "\n",
    "# loss value, and gradients for model's trainable parameters\n",
    "loss.item(), grads.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af473c33-d3f3-4d6c-b34c-a7fdfdd75901",
   "metadata": {},
   "source": [
    "### Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed96afc-83aa-4054-9da4-292f03dbf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525954e-0c05-401e-b4a1-08d23e3480c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use args.\n",
    "num_iters = 100\n",
    "batch_size = 4\n",
    "steps_per_report = 1\n",
    "steps_per_eval = 1\n",
    "n_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2fdd9-05af-43ad-9d0f-7cbcaafd3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [model.state, optimizer.state]\n",
    "\n",
    "# edit in qa.py\n",
    "# need here because of state variable\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def step(input_ids, token_type_ids, attention_mask, start_positions, end_positions):\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "    loss, grads = loss_and_grad_fn(\n",
    "        model, input_ids, token_type_ids, attention_mask, start_positions, end_positions)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4221c-7dc5-424c-b6cf-01f09066a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_iterator = batch_iterate(train_ds, batch_size=16)\n",
    "losses = []\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for it, batch in zip(range(num_iters), train_iterator):\n",
    "    print(it)\n",
    "    input_ids, token_type_ids, attention_mask, start_positions, end_positions = map(\n",
    "        mx.array,\n",
    "        (batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['start_positions'], batch['end_positions'])\n",
    "    )\n",
    "\n",
    "    loss = step(input_ids, token_type_ids, attention_mask, start_positions, end_positions)\n",
    "    mx.eval(state)\n",
    "    losses.append(loss.item())\n",
    "    if (it + 1) % steps_per_report == 0:\n",
    "        train_loss = np.mean(losses)\n",
    "        toc = time.perf_counter()\n",
    "        print(\n",
    "            f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "            f\"It/sec {steps_per_report / (toc - tic):.3f}\"\n",
    "        )\n",
    "        losses = []\n",
    "        tic = time.perf_counter()\n",
    "    if (it + 1) % steps_per_eval == 0:\n",
    "        val_loss = eval_fn(valid_ds, model, batch_size=batch_size)\n",
    "        toc = time.perf_counter()\n",
    "        print(\n",
    "            f\"Iter {it + 1}: \"\n",
    "            f\"Val loss {val_loss:.3f}, \"\n",
    "            f\"Val ppl {math.exp(val_loss):.3f}, \"\n",
    "            f\"Val took {(toc - tic):.3f}s, \"\n",
    "        )\n",
    "        tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476ac20-9898-4f7b-b5b9-5cf5d317334c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3773b-56ab-4a04-93b2-4954118d262e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17a05b-0353-43d8-8f8b-004cddbc47c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed313e-0220-48ae-b28c-ba3c39f82502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed743c0-7fdd-412c-8859-4044859c778f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99990e30-e5c5-4493-a54d-13a557c46974",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a8078-1806-41f3-9dec-02148128b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLX\n",
    "from transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizerBase\n",
    "from model import Bert\n",
    "\n",
    "bert_model = \"bert-base-uncased\"\n",
    "mlx_weights_path = \"weights/bert-base-uncased.npz\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(bert_model)\n",
    "model = Bert(config, add_pooler=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fe4d0-faad-4bc3-b7ac-5898313364e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch HF\n",
    "pre_train_model = bert_model\n",
    "tokenizerhf = BertTokenizerFast.from_pretrained(pre_train_model)\n",
    "modelhf = BertForQuestionAnswering.from_pretrained(pre_train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cca95c-183b-4a0a-98d0-653d24c15a1f",
   "metadata": {},
   "source": [
    "# MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2e3d0-5d87-4149-b797-d35c1d1c165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx.utils import tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6acd38-7b40-44f9-85ba-dabc72f32e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\"This is an example of BERT working on MLX.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afe2e5-90ac-4cc0-a47b-fc51fea78d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(batch, return_tensors=\"np\", padding=True)\n",
    "tokens = {key: mx.array(v) for key, v in tokens.items()}\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb837ac-2d5a-49a1-9763-a01b2f1a300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape, tokens['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb65cb-6413-49f6-88c0-2457705bbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(batch, return_tensors=\"mlx\", padding=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d194b41-3727-443d-afb9-a1ade43294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf4a17-2c62-4510-b1ea-9dcc3e5d456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape, tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce007e63-b1c5-4ad3-b158-57736a9ecc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca9cdf-dd9c-4788-b4df-0d4ce68f4371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef930f7-be8a-4bb3-b0fd-8273c1ff4ecf",
   "metadata": {},
   "source": [
    "# HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4c323-8239-41b1-a917-e6d95dc42af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel as BertModelHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9c16c-a14d-4d70-bc1c-e72accb1d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(bert_model)\n",
    "\n",
    "model = BertModelHF(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9437d-7f10-4a4f-9ac8-4a873dbe53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch2 = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9469d3a-4813-4d0a-b6f9-9d8b6c923a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "input_ids = batch2['input_ids'].to(device)\n",
    "token_type_ids = batch2['token_type_ids'].to(device)\n",
    "attention_mask = batch2['attention_mask'].to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31f93e-9d59-4d36-955b-7146558c70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0136efd1-89f9-4f20-a714-5eba06e7bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this seems to be the main output\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2896f-38cd-47b3-a087-262eaee8d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_qa_output = torch.nn.Linear(768, 2)\n",
    "self_qa_output.to(device)\n",
    "\n",
    "sequence_output = outputs[0]\n",
    "\n",
    "logits = self_qa_output(sequence_output)\n",
    "start_logits, end_logits = logits.split(1, dim=-1)\n",
    "# start_logits = start_logits.squeeze(-1)\n",
    "# end_logits = end_logits.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909eab6d-bc7a-4f02-8a74-37aa5ac2ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape, start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f46ba-a1ae-4449-9bc1-22238e431fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f43ed0-6fd5-4cb4-a53c-a277946d7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79749132-c9c5-4501-aea9-c91e5581e8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2407c-c0a2-40c2-97f7-b092ba266f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9021597f-7885-4f8b-b6a4-747545c0efd2",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5009ba-e13a-4541-8887-ac2f8dbf79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45c91a-463a-4c45-a7ff-173a83a10a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9f278c6-3cdb-4d87-96b8-595f3d3d084c",
   "metadata": {},
   "source": [
    "# example datasets\n",
    ">>> squad\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 50\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "})\n",
    ">>> tokenized_squad\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 50\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114c81d-d495-452a-8da8-dec1aa661708",
   "metadata": {},
   "source": [
    "### Pytorch model, input, output example\n",
    "\n",
    "\n",
    "```python\n",
    ">>> model\n",
    "BertForQuestionAnswering(\n",
    "  (bert): BertModel(\n",
    "    (embeddings): BertEmbeddings(\n",
    "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "      (position_embeddings): Embedding(512, 768)\n",
    "      (token_type_embeddings): Embedding(2, 768)\n",
    "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "    (encoder): BertEncoder(\n",
    "      (layer): ModuleList(\n",
    "        (0-11): 12 x BertLayer(\n",
    "          (attention): BertAttention(\n",
    "            (self): BertSdpaSelfAttention(\n",
    "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "            (output): BertSelfOutput(\n",
    "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (intermediate): BertIntermediate(\n",
    "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            (intermediate_act_fn): GELUActivation()\n",
    "          )\n",
    "          (output): BertOutput(\n",
    "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
    ")\n",
    ">>> \n",
    "batch = next(iter(train_dataloader))\n",
    ">>> batch\n",
    "{'input_ids': tensor([[  101,  1996, 13546,  ...,     0,     0,     0],\n",
    "        [  101,  2129,  2116,  ...,     0,     0,     0],\n",
    "        [  101, 19739,  6862,  ...,     0,     0,     0],\n",
    "        ...,\n",
    "        [  101,  2129,  2116,  ...,     0,     0,     0],\n",
    "        [  101,  1996, 26129,  ...,     0,     0,     0],\n",
    "        [  101,  1999,  2054,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        ...,\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        ...,\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([ 81, 267,  12, 149,  58,  80,  58,  74, 135,  98,  84, 107,  86,  28,\n",
    "         37,  57]), 'end_positions': tensor([ 83, 269,  15, 149,  61,  82,  63,  74, 142, 100,  86, 107,  88,  28,\n",
    "         38,  57])}\n",
    ">>> input_ids = batch['input_ids'].to(device)\n",
    ">>> attention_mask = batch['attention_mask'].to(device)\n",
    ">>> start_positions = batch['start_positions'].to(device)\n",
    ">>> end_positions = batch['end_positions'].to(device)\n",
    ">>> outputs = model(input_ids, attention_mask=attention_mask,\n",
    "...                             start_positions=start_positions, end_positions=end_positions)\n",
    "\n",
    ">>> outputs\n",
    "QuestionAnsweringModelOutput(loss=tensor(6.3726, device='mps:0', grad_fn=<DivBackward0>), start_logits=tensor([[-0.8564, -0.1199,  0.0125,  ...,  0.2456,  0.3330,  0.3860],\n",
    "        [-0.7941, -0.1668,  0.2791,  ...,  0.1567,  0.1839,  0.1895],\n",
    "        [-0.3875, -0.0241,  0.4691,  ...,  0.1442,  0.2343,  0.3314],\n",
    "        ...,\n",
    "        [-0.5861,  0.0238,  0.4390,  ...,  0.4266,  0.4844,  0.4618],\n",
    "        [-0.8158, -0.1809, -0.2249,  ...,  0.3569,  0.3794,  0.3319],\n",
    "        [-0.6281, -0.0430,  0.1375,  ...,  0.2855,  0.2233,  0.1889]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.3384, -0.1375, -0.0802,  ..., -0.0332, -0.0099, -0.0567],\n",
    "        [ 0.2627, -0.2120, -0.3897,  ...,  0.0227, -0.0094,  0.0472],\n",
    "        [ 0.6734,  0.2672, -0.0346,  ...,  0.0886,  0.1867,  0.1734],\n",
    "        ...,\n",
    "        [ 0.4112, -0.1477, -0.1991,  ...,  0.0158,  0.0874,  0.1071],\n",
    "        [ 0.5058,  0.0427,  0.2843,  ...,  0.0442,  0.0791,  0.0314],\n",
    "        [ 0.5014,  0.1437, -0.2467,  ...,  0.1379,  0.0093, -0.0738]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
    ">>> outputs['start_logits']\n",
    "tensor([[-0.8564, -0.1199,  0.0125,  ...,  0.2456,  0.3330,  0.3860],\n",
    "        [-0.7941, -0.1668,  0.2791,  ...,  0.1567,  0.1839,  0.1895],\n",
    "        [-0.3875, -0.0241,  0.4691,  ...,  0.1442,  0.2343,  0.3314],\n",
    "        ...,\n",
    "        [-0.5861,  0.0238,  0.4390,  ...,  0.4266,  0.4844,  0.4618],\n",
    "        [-0.8158, -0.1809, -0.2249,  ...,  0.3569,  0.3794,  0.3319],\n",
    "        [-0.6281, -0.0430,  0.1375,  ...,  0.2855,  0.2233,  0.1889]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>)\n",
    ">>> outputs['start_logits'].shape\n",
    "torch.Size([16, 512])\n",
    ">>> outputs.keys()\n",
    "odict_keys(['loss', 'start_logits', 'end_logits'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b261dd-a906-4ed7-9d4d-abdbd6da3c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-playgrounds",
   "language": "python",
   "name": "mlx-playgrounds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
