{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511be899-25e7-4cc8-a2ca-1b999b0e3557",
   "metadata": {},
   "source": [
    "# Fine Tune BERT for Q&A with Apple MLX\n",
    "\n",
    "and compare to PyTorch HuggingFace implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5f8237-7e3d-4684-9900-330f31139e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d4337f-4ecb-4630-94cc-db5e84bc7c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Envs/mlx-playgrounds/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9109b393-0060-4c3b-a3db-6567ca8a4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_squad\n",
    "from model import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987ef77-d384-469a-aeab-74b0053798e0",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e54d729-6e43-4392-8447-c6bab0dbcbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad = load_squad(filter_size=500)\n",
    "squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "061a8078-1806-41f3-9dec-02148128b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Envs/mlx-playgrounds/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# MLX\n",
    "from transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizerBase\n",
    "from model import Bert\n",
    "\n",
    "bert_model = \"bert-base-uncased\"\n",
    "mlx_weights_path = \"weights/bert-base-uncased.npz\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(bert_model)\n",
    "model = Bert(config, add_pooler=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fe4d0-faad-4bc3-b7ac-5898313364e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch HF\n",
    "pre_train_model = bert_model\n",
    "tokenizerhf = BertTokenizerFast.from_pretrained(pre_train_model)\n",
    "modelhf = BertForQuestionAnswering.from_pretrained(pre_train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cca95c-183b-4a0a-98d0-653d24c15a1f",
   "metadata": {},
   "source": [
    "# MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40e2e3d0-5d87-4149-b797-d35c1d1c165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx.utils import tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6acd38-7b40-44f9-85ba-dabc72f32e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\"This is an example of BERT working on MLX.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50afe2e5-90ac-4cc0-a47b-fc51fea78d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[101, 2023, 2003, ..., 2595, 1012, 102]], dtype=int64),\n",
       " 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 1, 1, 1]], dtype=int64)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(batch, return_tensors=\"np\", padding=True)\n",
    "tokens = {key: mx.array(v) for key, v in tokens.items()}\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fabb65cb-6413-49f6-88c0-2457705bbef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[101, 2023, 2003, ..., 2595, 1012, 102]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(batch, return_tensors=\"mlx\", padding=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d194b41-3727-443d-afb9-a1ade43294f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eabf4a17-2c62-4510-b1ea-9dcc3e5d456d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 13), array([[101, 2023, 2003, ..., 2595, 1012, 102]], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape, tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce007e63-b1c5-4ad3-b158-57736a9ecc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca9cdf-dd9c-4788-b4df-0d4ce68f4371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef930f7-be8a-4bb3-b0fd-8273c1ff4ecf",
   "metadata": {},
   "source": [
    "# HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76d4c323-8239-41b1-a917-e6d95dc42af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel as BertModelHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cce9c16c-a14d-4d70-bc1c-e72accb1d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Envs/mlx-playgrounds/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(bert_model)\n",
    "\n",
    "model = BertModelHF(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76b9437d-7f10-4a4f-9ac8-4a873dbe53b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2003,  2019,  2742,  1997, 14324,  2551,  2006, 19875,\n",
       "          2595,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch2 = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9469d3a-4813-4d0a-b6f9-9d8b6c923a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "input_ids = batch2['input_ids'].to(device)\n",
    "token_type_ids = batch2['token_type_ids'].to(device)\n",
    "attention_mask = batch2['attention_mask'].to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31f93e-9d59-4d36-955b-7146558c70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0136efd1-89f9-4f20-a714-5eba06e7bc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9749, -0.8778, -1.0495,  ...,  0.0232, -0.7137,  0.1091],\n",
       "         [-0.2013, -2.1369, -0.1064,  ..., -0.2749,  0.2540, -0.1586],\n",
       "         [-1.3305, -1.0369,  0.5428,  ..., -0.3688,  0.3484, -0.0307],\n",
       "         ...,\n",
       "         [-1.4051, -2.7570,  1.2217,  ...,  0.2561, -0.7478,  0.7259],\n",
       "         [-0.3601,  0.1444, -0.0155,  ..., -0.7146,  1.0159,  0.1831],\n",
       "         [-0.4500, -1.5011, -0.4606,  ..., -0.3835, -0.0132,  0.0782]]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this seems to be the main output\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4dd2896f-38cd-47b3-a087-262eaee8d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_qa_output = torch.nn.Linear(768, 2)\n",
    "self_qa_output.to(device)\n",
    "\n",
    "sequence_output = outputs[0]\n",
    "\n",
    "logits = self_qa_output(sequence_output)\n",
    "start_logits, end_logits = logits.split(1, dim=-1)\n",
    "# start_logits = start_logits.squeeze(-1)\n",
    "# end_logits = end_logits.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "909eab6d-bc7a-4f02-8a74-37aa5ac2ac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13, 2]), torch.Size([1, 13, 1]), torch.Size([1, 13, 1]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a25f46ba-a1ae-4449-9bc1-22238e431fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4024,  0.4357],\n",
       "         [-0.1407,  0.8876],\n",
       "         [ 0.3326,  0.5539],\n",
       "         [-0.2147,  0.6298],\n",
       "         [ 0.0588,  0.9387],\n",
       "         [-0.4208,  1.5841],\n",
       "         [ 0.2975,  0.1298],\n",
       "         [-0.1793,  0.6286],\n",
       "         [-0.2853,  1.3027],\n",
       "         [ 0.3102,  0.7564],\n",
       "         [-0.1742,  0.9697],\n",
       "         [-0.6204,  1.5335],\n",
       "         [-0.4409,  0.9782]]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f43ed0-6fd5-4cb4-a53c-a277946d7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79749132-c9c5-4501-aea9-c91e5581e8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2407c-c0a2-40c2-97f7-b092ba266f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9021597f-7885-4f8b-b6a4-747545c0efd2",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5009ba-e13a-4541-8887-ac2f8dbf79f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bert(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (norm): LayerNorm(768, eps=1e-12, affine=True)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers.0): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.1): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.2): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.3): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.4): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.5): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.6): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.7): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.8): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.9): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.10): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "    (layers.11): TransformerEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (key_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (value_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (ln2): LayerNorm(768, eps=1e-12, affine=True)\n",
       "      (linear1): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "      (linear2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(input_dims=768, output_dims=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e45c91a-463a-4c45-a7ff-173a83a10a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelhf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9f278c6-3cdb-4d87-96b8-595f3d3d084c",
   "metadata": {},
   "source": [
    "# example datasets\n",
    ">>> squad\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 50\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "})\n",
    ">>> tokenized_squad\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 50\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114c81d-d495-452a-8da8-dec1aa661708",
   "metadata": {},
   "source": [
    "### Pytorch model, input, output example\n",
    "\n",
    "\n",
    "```python\n",
    ">>> model\n",
    "BertForQuestionAnswering(\n",
    "  (bert): BertModel(\n",
    "    (embeddings): BertEmbeddings(\n",
    "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "      (position_embeddings): Embedding(512, 768)\n",
    "      (token_type_embeddings): Embedding(2, 768)\n",
    "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "    (encoder): BertEncoder(\n",
    "      (layer): ModuleList(\n",
    "        (0-11): 12 x BertLayer(\n",
    "          (attention): BertAttention(\n",
    "            (self): BertSdpaSelfAttention(\n",
    "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "            (output): BertSelfOutput(\n",
    "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (intermediate): BertIntermediate(\n",
    "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            (intermediate_act_fn): GELUActivation()\n",
    "          )\n",
    "          (output): BertOutput(\n",
    "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
    ")\n",
    ">>> \n",
    "batch = next(iter(train_dataloader))\n",
    ">>> batch\n",
    "{'input_ids': tensor([[  101,  1996, 13546,  ...,     0,     0,     0],\n",
    "        [  101,  2129,  2116,  ...,     0,     0,     0],\n",
    "        [  101, 19739,  6862,  ...,     0,     0,     0],\n",
    "        ...,\n",
    "        [  101,  2129,  2116,  ...,     0,     0,     0],\n",
    "        [  101,  1996, 26129,  ...,     0,     0,     0],\n",
    "        [  101,  1999,  2054,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        ...,\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        ...,\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([ 81, 267,  12, 149,  58,  80,  58,  74, 135,  98,  84, 107,  86,  28,\n",
    "         37,  57]), 'end_positions': tensor([ 83, 269,  15, 149,  61,  82,  63,  74, 142, 100,  86, 107,  88,  28,\n",
    "         38,  57])}\n",
    ">>> input_ids = batch['input_ids'].to(device)\n",
    ">>> attention_mask = batch['attention_mask'].to(device)\n",
    ">>> start_positions = batch['start_positions'].to(device)\n",
    ">>> end_positions = batch['end_positions'].to(device)\n",
    ">>> outputs = model(input_ids, attention_mask=attention_mask,\n",
    "...                             start_positions=start_positions, end_positions=end_positions)\n",
    "\n",
    ">>> outputs\n",
    "QuestionAnsweringModelOutput(loss=tensor(6.3726, device='mps:0', grad_fn=<DivBackward0>), start_logits=tensor([[-0.8564, -0.1199,  0.0125,  ...,  0.2456,  0.3330,  0.3860],\n",
    "        [-0.7941, -0.1668,  0.2791,  ...,  0.1567,  0.1839,  0.1895],\n",
    "        [-0.3875, -0.0241,  0.4691,  ...,  0.1442,  0.2343,  0.3314],\n",
    "        ...,\n",
    "        [-0.5861,  0.0238,  0.4390,  ...,  0.4266,  0.4844,  0.4618],\n",
    "        [-0.8158, -0.1809, -0.2249,  ...,  0.3569,  0.3794,  0.3319],\n",
    "        [-0.6281, -0.0430,  0.1375,  ...,  0.2855,  0.2233,  0.1889]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.3384, -0.1375, -0.0802,  ..., -0.0332, -0.0099, -0.0567],\n",
    "        [ 0.2627, -0.2120, -0.3897,  ...,  0.0227, -0.0094,  0.0472],\n",
    "        [ 0.6734,  0.2672, -0.0346,  ...,  0.0886,  0.1867,  0.1734],\n",
    "        ...,\n",
    "        [ 0.4112, -0.1477, -0.1991,  ...,  0.0158,  0.0874,  0.1071],\n",
    "        [ 0.5058,  0.0427,  0.2843,  ...,  0.0442,  0.0791,  0.0314],\n",
    "        [ 0.5014,  0.1437, -0.2467,  ...,  0.1379,  0.0093, -0.0738]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
    ">>> outputs['start_logits']\n",
    "tensor([[-0.8564, -0.1199,  0.0125,  ...,  0.2456,  0.3330,  0.3860],\n",
    "        [-0.7941, -0.1668,  0.2791,  ...,  0.1567,  0.1839,  0.1895],\n",
    "        [-0.3875, -0.0241,  0.4691,  ...,  0.1442,  0.2343,  0.3314],\n",
    "        ...,\n",
    "        [-0.5861,  0.0238,  0.4390,  ...,  0.4266,  0.4844,  0.4618],\n",
    "        [-0.8158, -0.1809, -0.2249,  ...,  0.3569,  0.3794,  0.3319],\n",
    "        [-0.6281, -0.0430,  0.1375,  ...,  0.2855,  0.2233,  0.1889]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>)\n",
    ">>> outputs['start_logits'].shape\n",
    "torch.Size([16, 512])\n",
    ">>> outputs.keys()\n",
    "odict_keys(['loss', 'start_logits', 'end_logits'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b261dd-a906-4ed7-9d4d-abdbd6da3c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-playgrounds",
   "language": "python",
   "name": "mlx-playgrounds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
