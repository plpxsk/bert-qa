{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511be899-25e7-4cc8-a2ca-1b999b0e3557",
   "metadata": {},
   "source": [
    "# Fine Tune BERT for Q&A with Apple MLX\n",
    "\n",
    "and compare to PyTorch HuggingFace implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5f8237-7e3d-4684-9900-330f31139e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "61d4337f-4ecb-4630-94cc-db5e84bc7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191f13de-862b-4f75-a34b-2d15d6c7e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9109b393-0060-4c3b-a3db-6567ca8a4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_processed_squad\n",
    "from qa import load_model_tokenizer, batch_iterate, loss_fn, eval_fn, build_parser\n",
    "\n",
    "# file from mlx repo\n",
    "from model_mlx import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987ef77-d384-469a-aeab-74b0053798e0",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2883799-3717-4e77-acab-765464251641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-base-uncased', 1000)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = build_parser()\n",
    "# pass empty string in jupyter\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.model_str, args.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d01d56a3-872a-485e-9618-b00ba522a1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('weights/bert-base-uncased.npz', 'weights/tmp-fine-tuned.npz')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.load_weights, args.save_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57227bd-38a2-4d68-badc-ad69d008f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for Bert()\n",
    "# batch = [\"This is an example of BERT working on MLX.\"]\n",
    "# tokens = tokenizer(batch, return_tensors=\"mlx\", padding=True)\n",
    "# output, pooled = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5292a8b7-992c-481a-990e-555a2713b78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Envs/mlx-playgrounds/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_tokenizer(hf_model=args.model_str,\n",
    "                                        weights_pretrain_path=args.load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b3c8d9dd-ecef-4c85-bf42-19be8ee93e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 2177.07 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 4181.79 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 4051.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((500, 5), (250, 5), (250, 5))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = load_processed_squad(filter_size=args.dataset_size,\n",
    "                                            model_max_length=tokenizer.model_max_length, tokenizer=tokenizer)\n",
    "\n",
    "train_ds.shape, valid_ds.shape, test_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "45914e2a-e792-4773-a4ec-5a6e6a58b2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(batch_iterate(train_ds, batch_size=10))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c06e0e9-1ade-4e8a-9a97-1bf0878507a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 512), (10, 512), (10, 512), array(1263.82, dtype=float32))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: mx.array() in preprocess_tokenize_function() ????\n",
    "input_ids, token_type_ids, attention_mask, start_positions, end_positions = map(\n",
    "    mx.array,\n",
    "    (batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['start_positions'], batch['end_positions'])\n",
    ")\n",
    "\n",
    "# input_ids = mx.expand_dims(input_ids, 0)\n",
    "# token_type_ids = mx.expand_dims(token_type_ids, 0)\n",
    "# attention_mask = mx.expand_dims(attention_mask, 0)\n",
    "\n",
    "input_ids.shape, token_type_ids.shape, attention_mask.shape, input_ids.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f457b87c-c1a6-43d0-81a1-5bc03dfc1eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10,), (10,))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_positions.shape, end_positions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d2ebb-6ee3-48a4-b1a7-72414ece7820",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75071e8-a0d8-418a-8ef2-0dc7d49802b9",
   "metadata": {},
   "source": [
    "### Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4d968cbf-e287-4ca1-afd6-6fc00a6f89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] how much money did the mrs. carter show limited edition fragrance make? [SEP] beyonce has worked with tommy hilfiger for the fragrances true star ( singing a cover version of \" wishing on a star \" ) and true star gold ; she also promoted emporio armani\\'s diamonds fragrance in 2007. beyonce launched her first official fragrance, heat in 2010. the commercial, which featured the 1956 song \" fever \", was shown after the water shed in the united kingdom as it begins with an image of beyonce appearing to lie naked in a room. in february 2011, beyonce launched her second fragrance, heat rush. beyonce\\'s third fragrance, pulse, was launched in september 2011. in 2013, the mrs. carter show limited edition version of heat was released. the six editions of heat are the world\\'s best - selling celebrity fragrance line, with sales of over $ 400 million. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently, this requires non-mlx inputs\n",
    "tokenizer.decode(np.array(input_ids[0]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3c9d6-892d-4fef-8f1f-218f765cf906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79117641-3720-483b-91f8-54392e5cc9c9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bde1bc-9fe0-4b2c-851a-cc6ca9d31b34",
   "metadata": {},
   "source": [
    "follow transformer_lm/main/py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7c5c6ec-fe0a-443a-aa05-fdd955e8f10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c24f7-ac4c-46ac-a3ba-8046accb10a0",
   "metadata": {},
   "source": [
    "### Train on single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03015069-242a-4024-bdba-f4491502c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "567d0650-1e91-4789-804b-eb89c4cfcf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e95d02c-949f-4282-9964-002fa77a5d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10,), (10,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits, end_logits = model(\n",
    "    input_ids=input_ids,\n",
    "    token_type_ids=token_type_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    start_positions=start_positions,\n",
    "    end_positions=end_positions) \n",
    "\n",
    "a = nn.losses.cross_entropy(start_logits, start_positions)\n",
    "b = nn.losses.cross_entropy(end_logits, end_positions)\n",
    "\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e9c75c8-a0ec-4790-87d7-bb12fb7274f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.27944, 6.1827, 6.67555, ..., 6.17367, 6.08465, 6.76645], dtype=float32),\n",
       " array([5.7783, 6.2521, 6.27144, ..., 5.73106, 6.10344, 6.12746], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b152c813-a373-4988-9229-513d2ffaca07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.18729, dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model, input_ids, token_type_ids, attention_mask, start_positions, end_positions, reduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce8afeb8-9210-40fa-99ba-0debaed0ea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.02887, 6.2174, 6.4735, ..., 5.95237, 6.09405, 6.44696], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model, input_ids, token_type_ids, attention_mask, start_positions, end_positions, reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4455b85-169b-4e94-ba26-2f08fd553229",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ece8f5d-7d3c-404d-93d8-ab2311912d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.187294006347656, dict_keys(['model', 'qa_output']))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, grads = loss_and_grad_fn(model, input_ids, token_type_ids, attention_mask, start_positions, end_positions)\n",
    "\n",
    "# loss value, and gradients for model's trainable parameters\n",
    "loss.item(), grads.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af473c33-d3f3-4d6c-b34c-a7fdfdd75901",
   "metadata": {},
   "source": [
    "### Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ed96afc-83aa-4054-9da4-292f03dbf5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4525954e-0c05-401e-b4a1-08d23e3480c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use args.\n",
    "num_iters = 20\n",
    "batch_size = 10\n",
    "steps_per_report = 10\n",
    "steps_per_eval = 10\n",
    "n_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1c2fdd9-05af-43ad-9d0f-7cbcaafd3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [model.state, optimizer.state]\n",
    "\n",
    "# edit in qa.py\n",
    "# need here because of state variable\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def step(input_ids, token_type_ids, attention_mask, start_positions, end_positions):\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "    loss, grads = loss_and_grad_fn(\n",
    "        model, input_ids, token_type_ids, attention_mask, start_positions, end_positions)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35a4221c-7dc5-424c-b6cf-01f09066a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10: Train loss 3.420, It/sec 0.923\n",
      "Iter 10: Val loss 4.030, Val ppl 56.280, Val took 8.778s, \n",
      "Iter 20: Train loss 3.598, It/sec 0.927\n",
      "Iter 20: Val loss 3.699, Val ppl 40.426, Val took 8.885s, \n"
     ]
    }
   ],
   "source": [
    "train_iterator = batch_iterate(train_ds, batch_size=batch_size)\n",
    "losses = []\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for it, batch in zip(range(num_iters), train_iterator):\n",
    "    input_ids, token_type_ids, attention_mask, start_positions, end_positions = map(\n",
    "        mx.array,\n",
    "        (batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['start_positions'], batch['end_positions'])\n",
    "    )\n",
    "\n",
    "    loss = step(input_ids, token_type_ids, attention_mask, start_positions, end_positions)\n",
    "    mx.eval(state)\n",
    "    losses.append(loss.item())\n",
    "    if (it + 1) % steps_per_report == 0:\n",
    "        train_loss = np.mean(losses)\n",
    "        toc = time.perf_counter()\n",
    "        print(\n",
    "            f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "            f\"It/sec {steps_per_report / (toc - tic):.3f}\"\n",
    "        )\n",
    "        losses = []\n",
    "        tic = time.perf_counter()\n",
    "    if (it + 1) % steps_per_eval == 0:\n",
    "        val_loss = eval_fn(valid_ds, model, batch_size=batch_size)\n",
    "        toc = time.perf_counter()\n",
    "        print(\n",
    "            f\"Iter {it + 1}: \"\n",
    "            f\"Val loss {val_loss:.3f}, \"\n",
    "            f\"Val ppl {math.exp(val_loss):.3f}, \"\n",
    "            f\"Val took {(toc - tic):.3f}s, \"\n",
    "        )\n",
    "        tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b88cb1-9b2c-47a9-98fc-dc1f0028271c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3095e01-d2b1-465a-b95e-d33b046e1970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caad33a7-9048-4c38-aeb0-79157edc6274",
   "metadata": {},
   "source": [
    "# Saving and loading new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef81be32-b825-4fc2-80e5-541e34ebd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx.utils import tree_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b545e77-47e3-41e0-8115-bd3cae7fb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 108.893M\n",
      "Trainable parameters 108.893M\n"
     ]
    }
   ],
   "source": [
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ab3773b-56ab-4a04-93b2-4954118d262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"tmp-fine-tuned.npz\"\n",
    "# mx.savez(f, **dict(tree_flatten(model.trainable_parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34af3b-11f4-4d9a-b1bd-66ffec1dd170",
   "metadata": {},
   "source": [
    "# Compare fine-tuned to raw model\n",
    "To show that fine tuning did something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dedfc873-348e-44b4-b993-d6c90ba46e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_mlx import BertQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2a0bae33-e01f-4c51-8594-b6b66e8b006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Envs/mlx-playgrounds/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_pre, tokenizer = load_model_tokenizer(hf_model=args.model_str,\n",
    "                                            weights_pretrain_path=args.load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "775a76c3-4316-43c8-baf7-98f10db90e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"weights/fine-tuned-tiny.npz\"\n",
    "model_fine, _ = load_model_tokenizer(hf_model=args.model_str,\n",
    "                                     weights_finetuned_path=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6eb536a-f2f4-4937-bb95-92ce39ee67a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 108.893M\n",
      "Trainable parameters 108.893M\n",
      "Total parameters 108.893M\n",
      "Trainable parameters 108.893M\n"
     ]
    }
   ],
   "source": [
    "p = sum(v.size for _, v in tree_flatten(model_pre.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model_pre.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model_fine.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model_fine.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce052745-d207-4350-8fa5-54fc68bcc5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "722fe7a8-1fdc-462f-a9ef-ab68b8d21226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.280390125274658, 533.9969489006398)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_pre\n",
    "\n",
    "test_loss = eval_fn(test_ds, model, batch_size=args.batch_size)\n",
    "test_loss, math.exp(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4756bcd5-7145-4eff-b559-592860e6bf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.391675977706909, 29.715713432679717)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_fine\n",
    "\n",
    "test_loss = eval_fn(test_ds, model, batch_size=args.batch_size)\n",
    "test_loss, math.exp(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9820ef-8c1c-42db-ab94-6294d8aa7cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b96b4-d380-4c53-809d-579afbc5c86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e36454de-d00e-45fe-b416-a53e0eca29e2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f0e5759a-b766-426e-85d3-3e7aa08f720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_valid_answers, find_context_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4a855ad4-2f70-4a99-b2b1-450fb26fb8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BertTokenizerFast(name_or_path='bert-base-uncased'\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = \"weights/fine-tuned-tiny.npz\"\n",
    "model, tokenizer = load_model_tokenizer(hf_model=args.model_str,\n",
    "                                        weights_finetuned_path=f)\n",
    "str(tokenizer)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c632e61d-9549-4d17-8c5c-7b7336cd55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"HF Transformers is backed by the three most popular deep learning libraries - Jax, PyTorch and TensorFlow - with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other\"\n",
    "question = \"Which deep learning libraries back HF Transformers?\"\n",
    "\n",
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f39666f8-2593-4840-961b-9347f6c51ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(question, context, return_tensors=\"mlx\")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c3147e6-9af3-4ea4-9473-7fa4e8ec9549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] how many programming languages does bloom support? [SEP] bloom has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages. [SEP]'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to decode, need to use HF\n",
    "tokenizer.decode(np.array(inputs['input_ids']).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d602fc6-d91b-4f36-8969-8dc5ac97d290",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d4071-9e58-45de-b04e-c6e31daa6d7a",
   "metadata": {},
   "source": [
    "### Find valid answers\n",
    "follow find_valid_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6ad59a93-1dd3-4931-95be-2633423db897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.sequence_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c2a16024-2da3-4647-b2a6-d82d1a19806d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "context_start_index, context_end_index = find_context_start_end(\n",
    "        inputs.sequence_ids())\n",
    "\n",
    "context_start_index, context_end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1430d843-00ad-4293-a57c-272afa9eb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dims like this\n",
    "# len(start_logits.shape), len(start_logits.flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a9f7ed1c-ed89-44d9-a68c-1263c5a3ee08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19,), (19,))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits, end_logits = model(**inputs)\n",
    "start_logits.shape, end_logits.shape\n",
    "\n",
    "# flatten first to extrac\n",
    "start_logits = start_logits.flatten()[context_start_index: context_end_index + 1]\n",
    "end_logits = end_logits.flatten()[context_start_index: context_end_index + 1]\n",
    "\n",
    "start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c68cd044-367b-433b-8c68-3c4fe8500419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_best_size = 5\n",
    "top_k = min(n_best_size, len(start_logits))\n",
    "\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c67348a3-95f1-4658-ad6b-03be432b5c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 10, 15, 3, 0], [3, 4, 2, 10, 15])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_start_indices = np.argsort(start_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "topk_end_indices = np.argsort(end_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "\n",
    "topk_start_indices, topk_end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "22036d73-9550-4dd0-be5c-63ce19ddfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answers = []\n",
    "\n",
    "# score all top logits\n",
    "for start in topk_start_indices:\n",
    "    for end in topk_end_indices:\n",
    "        if start <= end:\n",
    "            valid_answers.append({\n",
    "                \"score\": start_logits[start] + end_logits[end],\n",
    "                # shift indeces back to input-zero'd\n",
    "                \"start\": start + context_start_index,\n",
    "                \"end\": end + context_start_index\n",
    "            })\n",
    "valid_answers.sort(key=lambda x: x['score'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "84a4a1c7-51e3-4611-864b-c2a423a77db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': array(10.2414, dtype=float32), 'start': 12, 'end': 13},\n",
       " {'score': array(9.44422, dtype=float32), 'start': 12, 'end': 14},\n",
       " {'score': array(8.82822, dtype=float32), 'start': 12, 'end': 12},\n",
       " {'score': array(8.22495, dtype=float32), 'start': 13, 'end': 13},\n",
       " {'score': array(8.077, dtype=float32), 'start': 12, 'end': 20},\n",
       " {'score': array(8.03333, dtype=float32), 'start': 12, 'end': 25},\n",
       " {'score': array(7.88629, dtype=float32), 'start': 10, 'end': 13},\n",
       " {'score': array(7.42905, dtype=float32), 'start': 20, 'end': 20},\n",
       " {'score': array(7.42777, dtype=float32), 'start': 13, 'end': 14},\n",
       " {'score': array(7.38538, dtype=float32), 'start': 20, 'end': 25},\n",
       " {'score': array(7.08911, dtype=float32), 'start': 10, 'end': 14},\n",
       " {'score': array(6.47311, dtype=float32), 'start': 10, 'end': 12},\n",
       " {'score': array(6.4699, dtype=float32), 'start': 25, 'end': 25},\n",
       " {'score': array(6.06056, dtype=float32), 'start': 13, 'end': 20},\n",
       " {'score': array(6.01688, dtype=float32), 'start': 13, 'end': 25},\n",
       " {'score': array(5.72189, dtype=float32), 'start': 10, 'end': 20},\n",
       " {'score': array(5.67822, dtype=float32), 'start': 10, 'end': 25}]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3f709832-c5c3-4ef6-8877-b391a1799e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " How many programming languages does BLOOM support?\n",
      "array(10.2414, dtype=float32)\n",
      "176 billion\n",
      "array(9.44422, dtype=float32)\n",
      "176 billion parameters\n",
      "array(8.82822, dtype=float32)\n",
      "176\n",
      "array(8.22495, dtype=float32)\n",
      "billion\n",
      "array(8.077, dtype=float32)\n",
      "176 billion parameters and can generate text in 46\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", question)\n",
    "for d in valid_answers[:5]:\n",
    "    score = d['score']\n",
    "    start = d['start']\n",
    "    end = d['end']\n",
    "    print(score)\n",
    "    predict_answer_tokens = inputs.input_ids[0, start: end + 1]\n",
    "    print(tokenizer.decode(np.array(predict_answer_tokens.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f587ad7-d882-4b5a-9173-b952ce05107a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b975f2-a6ef-40e9-91fa-5fac00c8ab84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd103fc-b7ec-4cfa-a8d9-2c1559beb3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167f7a8-64d3-4de3-90e1-f1ff964afc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125274d7-2e0e-4644-b4a4-16c58d4fa513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51675371-aeb1-4794-a7a3-4e781a7316aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17a05b-0353-43d8-8f8b-004cddbc47c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed313e-0220-48ae-b28c-ba3c39f82502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed743c0-7fdd-412c-8859-4044859c778f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99990e30-e5c5-4493-a54d-13a557c46974",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a8078-1806-41f3-9dec-02148128b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLX\n",
    "from transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizerBase\n",
    "from model import Bert\n",
    "\n",
    "bert_model = \"bert-base-uncased\"\n",
    "mlx_weights_path = \"weights/bert-base-uncased.npz\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(bert_model)\n",
    "model = Bert(config, add_pooler=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fe4d0-faad-4bc3-b7ac-5898313364e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch HF\n",
    "pre_train_model = bert_model\n",
    "tokenizerhf = BertTokenizerFast.from_pretrained(pre_train_model)\n",
    "modelhf = BertForQuestionAnswering.from_pretrained(pre_train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cca95c-183b-4a0a-98d0-653d24c15a1f",
   "metadata": {},
   "source": [
    "# MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2e3d0-5d87-4149-b797-d35c1d1c165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx.utils import tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6acd38-7b40-44f9-85ba-dabc72f32e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\"This is an example of BERT working on MLX.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afe2e5-90ac-4cc0-a47b-fc51fea78d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(batch, return_tensors=\"np\", padding=True)\n",
    "tokens = {key: mx.array(v) for key, v in tokens.items()}\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb837ac-2d5a-49a1-9763-a01b2f1a300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape, tokens['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb65cb-6413-49f6-88c0-2457705bbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(batch, return_tensors=\"mlx\", padding=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d194b41-3727-443d-afb9-a1ade43294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf4a17-2c62-4510-b1ea-9dcc3e5d456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape, tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce007e63-b1c5-4ad3-b158-57736a9ecc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca9cdf-dd9c-4788-b4df-0d4ce68f4371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef930f7-be8a-4bb3-b0fd-8273c1ff4ecf",
   "metadata": {},
   "source": [
    "# HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4c323-8239-41b1-a917-e6d95dc42af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel as BertModelHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9c16c-a14d-4d70-bc1c-e72accb1d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(bert_model)\n",
    "\n",
    "model = BertModelHF(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "76b9437d-7f10-4a4f-9ac8-4a873dbe53b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch2 \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m batch2\n",
      "File \u001b[0;32m~/Envs/mlx-playgrounds/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3055\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3055\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/Envs/mlx-playgrounds/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 3114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3117\u001b[0m     )\n\u001b[1;32m   3119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   3120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3123\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "batch2 = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9469d3a-4813-4d0a-b6f9-9d8b6c923a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "input_ids = batch2['input_ids'].to(device)\n",
    "token_type_ids = batch2['token_type_ids'].to(device)\n",
    "attention_mask = batch2['attention_mask'].to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31f93e-9d59-4d36-955b-7146558c70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0136efd1-89f9-4f20-a714-5eba06e7bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this seems to be the main output\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2896f-38cd-47b3-a087-262eaee8d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_qa_output = torch.nn.Linear(768, 2)\n",
    "self_qa_output.to(device)\n",
    "\n",
    "sequence_output = outputs[0]\n",
    "\n",
    "logits = self_qa_output(sequence_output)\n",
    "start_logits, end_logits = logits.split(1, dim=-1)\n",
    "# start_logits = start_logits.squeeze(-1)\n",
    "# end_logits = end_logits.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909eab6d-bc7a-4f02-8a74-37aa5ac2ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape, start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f46ba-a1ae-4449-9bc1-22238e431fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f43ed0-6fd5-4cb4-a53c-a277946d7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79749132-c9c5-4501-aea9-c91e5581e8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2407c-c0a2-40c2-97f7-b092ba266f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9021597f-7885-4f8b-b6a4-747545c0efd2",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5009ba-e13a-4541-8887-ac2f8dbf79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45c91a-463a-4c45-a7ff-173a83a10a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9f278c6-3cdb-4d87-96b8-595f3d3d084c",
   "metadata": {},
   "source": [
    "# example datasets\n",
    ">>> squad\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 50\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "})\n",
    ">>> tokenized_squad\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 50\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
    "        num_rows: 25\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114c81d-d495-452a-8da8-dec1aa661708",
   "metadata": {},
   "source": [
    "### Pytorch model, input, output example\n",
    "\n",
    "\n",
    "```python\n",
    ">>> model\n",
    "BertForQuestionAnswering(\n",
    "  (bert): BertModel(\n",
    "    (embeddings): BertEmbeddings(\n",
    "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "      (position_embeddings): Embedding(512, 768)\n",
    "      (token_type_embeddings): Embedding(2, 768)\n",
    "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "    (encoder): BertEncoder(\n",
    "      (layer): ModuleList(\n",
    "        (0-11): 12 x BertLayer(\n",
    "          (attention): BertAttention(\n",
    "            (self): BertSdpaSelfAttention(\n",
    "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "            (output): BertSelfOutput(\n",
    "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (intermediate): BertIntermediate(\n",
    "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            (intermediate_act_fn): GELUActivation()\n",
    "          )\n",
    "          (output): BertOutput(\n",
    "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
    ")\n",
    ">>> \n",
    "batch = next(iter(train_dataloader))\n",
    ">>> batch\n",
    "{'input_ids': tensor([[  101,  1996, 13546,  ...,     0,     0,     0],\n",
    "        [  101,  2129,  2116,  ...,     0,     0,     0],\n",
    "        [  101, 19739,  6862,  ...,     0,     0,     0],\n",
    "        ...,\n",
    "        [  101,  2129,  2116,  ...,     0,     0,     0],\n",
    "        [  101,  1996, 26129,  ...,     0,     0,     0],\n",
    "        [  101,  1999,  2054,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        ...,\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0],\n",
    "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        ...,\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0],\n",
    "        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([ 81, 267,  12, 149,  58,  80,  58,  74, 135,  98,  84, 107,  86,  28,\n",
    "         37,  57]), 'end_positions': tensor([ 83, 269,  15, 149,  61,  82,  63,  74, 142, 100,  86, 107,  88,  28,\n",
    "         38,  57])}\n",
    ">>> input_ids = batch['input_ids'].to(device)\n",
    ">>> attention_mask = batch['attention_mask'].to(device)\n",
    ">>> start_positions = batch['start_positions'].to(device)\n",
    ">>> end_positions = batch['end_positions'].to(device)\n",
    ">>> outputs = model(input_ids, attention_mask=attention_mask,\n",
    "...                             start_positions=start_positions, end_positions=end_positions)\n",
    "\n",
    ">>> outputs\n",
    "QuestionAnsweringModelOutput(loss=tensor(6.3726, device='mps:0', grad_fn=<DivBackward0>), start_logits=tensor([[-0.8564, -0.1199,  0.0125,  ...,  0.2456,  0.3330,  0.3860],\n",
    "        [-0.7941, -0.1668,  0.2791,  ...,  0.1567,  0.1839,  0.1895],\n",
    "        [-0.3875, -0.0241,  0.4691,  ...,  0.1442,  0.2343,  0.3314],\n",
    "        ...,\n",
    "        [-0.5861,  0.0238,  0.4390,  ...,  0.4266,  0.4844,  0.4618],\n",
    "        [-0.8158, -0.1809, -0.2249,  ...,  0.3569,  0.3794,  0.3319],\n",
    "        [-0.6281, -0.0430,  0.1375,  ...,  0.2855,  0.2233,  0.1889]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.3384, -0.1375, -0.0802,  ..., -0.0332, -0.0099, -0.0567],\n",
    "        [ 0.2627, -0.2120, -0.3897,  ...,  0.0227, -0.0094,  0.0472],\n",
    "        [ 0.6734,  0.2672, -0.0346,  ...,  0.0886,  0.1867,  0.1734],\n",
    "        ...,\n",
    "        [ 0.4112, -0.1477, -0.1991,  ...,  0.0158,  0.0874,  0.1071],\n",
    "        [ 0.5058,  0.0427,  0.2843,  ...,  0.0442,  0.0791,  0.0314],\n",
    "        [ 0.5014,  0.1437, -0.2467,  ...,  0.1379,  0.0093, -0.0738]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
    ">>> outputs['start_logits']\n",
    "tensor([[-0.8564, -0.1199,  0.0125,  ...,  0.2456,  0.3330,  0.3860],\n",
    "        [-0.7941, -0.1668,  0.2791,  ...,  0.1567,  0.1839,  0.1895],\n",
    "        [-0.3875, -0.0241,  0.4691,  ...,  0.1442,  0.2343,  0.3314],\n",
    "        ...,\n",
    "        [-0.5861,  0.0238,  0.4390,  ...,  0.4266,  0.4844,  0.4618],\n",
    "        [-0.8158, -0.1809, -0.2249,  ...,  0.3569,  0.3794,  0.3319],\n",
    "        [-0.6281, -0.0430,  0.1375,  ...,  0.2855,  0.2233,  0.1889]],\n",
    "       device='mps:0', grad_fn=<CloneBackward0>)\n",
    ">>> outputs['start_logits'].shape\n",
    "torch.Size([16, 512])\n",
    ">>> outputs.keys()\n",
    "odict_keys(['loss', 'start_logits', 'end_logits'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b261dd-a906-4ed7-9d4d-abdbd6da3c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-playgrounds",
   "language": "python",
   "name": "mlx-playgrounds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
